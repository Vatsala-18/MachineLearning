{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10_NLP(part2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPQQ6P4sa9qD0qIGTi+veG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vatsala-18/MachineLearning/blob/main/10_NLP(part2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFLohsa4_FtV"
      },
      "source": [
        "import joblib \n",
        "text_model = joblib.load('spam-ham')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUlrjURM_hcr",
        "outputId": "518c2cea-0b36-4040-ac0b-ba5d91afac2b"
      },
      "source": [
        "!pip install streamlit --quiet "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 8.3 MB 6.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 62.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 76 kB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 38.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 111 kB 61.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 124 kB 56.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 788 kB 55.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 370 kB 68.6 MB/s \n",
            "\u001b[?25h  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.20 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.4.1 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.28.0 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGIeJNDPCXY6",
        "outputId": "1112c162-3f98-4c93-d239-8a3301ec23e9"
      },
      "source": [
        "!pip install pyngrok==4.1.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok==4.1.1\n",
            "  Downloading pyngrok-4.1.1.tar.gz (18 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (0.16.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (3.13)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.1-py3-none-any.whl size=15984 sha256=b1dc1a261105bceb7583481cf7abb95d426596206009d546acec82c8a381cde8\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/d9/12/045a042fee3127dc40ba6f5df2798aa2df38c414bf533ca765\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAgl-XOzCik6"
      },
      "source": [
        "from pyngrok import ngrok"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k586Vc8iCsY0",
        "outputId": "93ee6daf-2277-43b2-ce5b-7240575b3a2b"
      },
      "source": [
        "%%writefile app.py \n",
        "import streamlit as st \n",
        "import joblib\n",
        "st.title('SPAM HAM CLASSIFICATION')  #title to the web app\n",
        "text_model = joblib.load('spam-ham') #load the model back using joblib \n",
        "ip = st.text_input('Enter your message :')  #user input in streamlit \n",
        "op = text_model.predict([ip])           #predict if the written message is spam or ham \n",
        "if st.button('Predict'):   #creating a button called as predict\n",
        "  st.title(op[0])  #gives the output in the app"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "eMk0FTDUCzSD",
        "outputId": "96c2d18f-26e7-44c5-a21d-ce1d37054e18"
      },
      "source": [
        "!nohup streamlit run app.py & \n",
        "public_url = ngrok.connect(port = '8501')\n",
        "public_url"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'http://e1bc-35-231-152-161.ngrok.io'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDX4N1cMC3kl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HVXqX_eEOkN"
      },
      "source": [
        "**STEMMING AND LEMMATIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_LY0T_NERQc"
      },
      "source": [
        "# Stopwords are the English words which does not add much meaning to a sentence. \n",
        "# They can safely be ignored without sacrificing the meaning of the sentence"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7yhUkRBEa_U",
        "outputId": "5568e371-ce2f-41c7-f0d6-58142cdf8953"
      },
      "source": [
        "import nltk #natural language toolkit library \n",
        "nltk.download('stopwords')  #stop words \n",
        "nltk.download('wordnet')  #lemmatizer \n",
        "nltk.download('punkt')   #tokenizer "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju6AQ0eREjS2"
      },
      "source": [
        "#LEMMATIZATION\n",
        "import re                  #regular expression\n",
        "from nltk.corpus import stopwords     #corpus has a lot of natural language datsets\n",
        "from nltk.stem import WordNetLemmatizer  \n",
        "lemmatizer = WordNetLemmatizer()    #calling the lemmatizer\n",
        "\n",
        "\n",
        "corpus_lem =[]  #creating an empty list \n",
        "\n",
        "\n",
        "x = \"He was running and eating at the same time. He has bad habit of swimming after playing long hours in the sun \"\n",
        "review = re.sub('[^a-zA-Z]',' ',x)  #if it finds anything which is not text, replace it without any spacing and give the output\n",
        "review = review.lower() #convert all text into lowercase\n",
        "review = review.split() #to split/tokenize \n",
        "\n",
        "review_lem = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')] #using lemmatizer, we are telling that if any word is found in the stopwords, then remove it from the sentence\n",
        "review_lem = ' '.join(review_lem)  #join the text in the sentence \n",
        "corpus_lem.append(review_lem)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwQBYykZEwu4"
      },
      "source": [
        "#STEMMING \n",
        "import re                  #regular expression\n",
        "from nltk.corpus import stopwords     #corpus has a lot of natural language datsets\n",
        "from nltk.stem.porter import PorterStemmer  \n",
        "stemmer = PorterStemmer()    #calling the stemmer\n",
        "\n",
        "\n",
        "corpus_stem =[]  #creating an empty list \n",
        "\n",
        "\n",
        "x = \"He was running and eating at the same time. He has bad habit of swimming after playing long hours in the sun \"\n",
        "review = re.sub('[^a-zA-Z]',' ',x)  #if it finds anything which is not text, replace it without any spacing and give the output\n",
        "review = review.lower() #convert all text into lowercase\n",
        "review = review.split() #to split/tokenize \n",
        "\n",
        "review_stem = [stemmer.stem(word) for word in review if not word in stopwords.words('english')] #using lemmatizer, we are telling that if any word is found in the stopwords, then remove it from the sentence\n",
        "review_stem = ' '.join(review_stem)  #join the text in the sentence \n",
        "corpus_stem.append(review_stem)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX_uASisE3a3",
        "outputId": "6b338adf-2372-44ef-83d2-76138da649bf"
      },
      "source": [
        "corpus_lem"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['running eating time bad habit swimming playing long hour sun']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa08fzFJE8WA",
        "outputId": "47e753c8-65eb-47bd-b2e2-8077abaa5a4e"
      },
      "source": [
        "corpus_stem"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['run eat time bad habit swim play long hour sun']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pnchdmlFBP_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}